{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import skseq\n",
    "from skseq.sequences.extended_feature import ExtendedFeatures\n",
    "import utils\n",
    "import skseq.sequences.structured_perceptron as spc\n",
    "\n",
    "currentdir = Path.cwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "N_EPOCHS = 15 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a sequence with each sentence, x having every word in a string and the tags in another vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = utils.gen_set(\"./data/train_data_ner.csv\")\n",
    "X_test, y_test = utils.gen_set(\"./data/test_data_ner.csv\")\n",
    "X_tiny, y_tiny = utils.gen_set(\"./data/tiny_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a sequence list, first we need a dictionary for the words and another for the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, tag_dict, rev_dict = utils.dictionary(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the class SequenceList from the skseq package used in the code provided in class. This package also includes a class for dictionaries called LabelDictionary defines some useful functions needed for creating the sequence, so we need to transform our dictionaries into these ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = utils.get_seq(word_dict, tag_dict, X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper = skseq.sequences.id_feature.IDFeatures(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.893522\n",
      "Epoch: 1 Accuracy: 0.931903\n",
      "Epoch: 2 Accuracy: 0.941308\n",
      "Epoch: 3 Accuracy: 0.946066\n",
      "Epoch: 4 Accuracy: 0.949996\n",
      "Epoch: 5 Accuracy: 0.952464\n",
      "Epoch: 6 Accuracy: 0.954540\n",
      "Epoch: 7 Accuracy: 0.956122\n",
      "Epoch: 8 Accuracy: 0.957765\n",
      "Epoch: 9 Accuracy: 0.957984\n",
      "Epoch: 10 Accuracy: 0.959716\n",
      "Epoch: 11 Accuracy: 0.959939\n",
      "Epoch: 12 Accuracy: 0.960999\n",
      "Epoch: 13 Accuracy: 0.961453\n",
      "Epoch: 14 Accuracy: 0.962112\n"
     ]
    }
   ],
   "source": [
    "sp = spc.StructuredPerceptron(word_dict, tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5\n",
    "pred_train = sp.viterbi_decode_corpus(seq)\n",
    "sp.fit(feature_mapper.dataset, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_model(\"./fitted_models/sp_base_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper_extra = ExtendedFeatures(seq)\n",
    "feature_mapper_extra.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spc.StructuredPerceptron(word_dict, tag_dict, feature_mapper_extra)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sp.viterbi_decode_corpus(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.928059\n",
      "Epoch: 1 Accuracy: 0.943572\n",
      "Epoch: 2 Accuracy: 0.947625\n",
      "Epoch: 3 Accuracy: 0.949844\n",
      "Epoch: 4 Accuracy: 0.951875\n",
      "Epoch: 5 Accuracy: 0.953769\n",
      "Epoch: 6 Accuracy: 0.954850\n",
      "Epoch: 7 Accuracy: 0.955497\n",
      "Epoch: 8 Accuracy: 0.956710\n",
      "Epoch: 9 Accuracy: 0.957138\n",
      "Epoch: 10 Accuracy: 0.957752\n",
      "Epoch: 11 Accuracy: 0.958760\n",
      "Epoch: 12 Accuracy: 0.958912\n",
      "Epoch: 13 Accuracy: 0.959638\n",
      "Epoch: 14 Accuracy: 0.960259\n"
     ]
    }
   ],
   "source": [
    "sp.fit(feature_mapper_extra.dataset, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_model(\"./fitted_models/sp_ext_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torcheval peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torcheval.metrics\n",
    "from utils import parse_dataset, Params, NERDataset, get_evaluation_pred_and_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = './data/'\n",
    "training = 'train_data_ner.csv'\n",
    "test = 'test_data_ner.csv'\n",
    "tiny = 'tiny_test.csv'\n",
    "\n",
    "full_train, full_train_label = parse_dataset(base_folder + training)\n",
    "len_train = int(len(full_train) * 0.9)\n",
    "\n",
    "train_sent, train_label = full_train[:len_train], full_train_label[:len_train]\n",
    "valid_sent, valid_label = full_train[len_train + 1:], full_train_label[len_train + 1:]\n",
    "\n",
    "test_sent, test_label = parse_dataset(base_folder + test)\n",
    "tiny_sent, tiny_label = parse_dataset(base_folder + tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organized_list = []\n",
    "for inner_list in train_label + test_label + tiny_label:\n",
    "    organized_list.extend(inner_list)\n",
    "set(organized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 10,\n",
    "    'lr': 3e-5,\n",
    "    'shuffle': False,\n",
    "    'weight_decay': 1e-4,\n",
    "    'embeddings_dim': 1024,\n",
    "}\n",
    "\n",
    "params = Params(PARAMS)\n",
    "\n",
    "label2id = {\n",
    "    'B-art': 0,\n",
    "    'B-eve': 1,\n",
    "    'B-geo': 2,\n",
    "    'B-gpe': 3,\n",
    "    'B-nat': 4,\n",
    "    'B-org': 5,\n",
    "    'B-per': 6,\n",
    "    'B-tim': 7,\n",
    "    'I-art': 8,\n",
    "    'I-eve': 9,\n",
    "    'I-geo': 10,\n",
    "    'I-gpe': 11,\n",
    "    'I-nat': 12,\n",
    "    'I-org': 13,\n",
    "    'I-per': 14,\n",
    "    'I-tim': 15,\n",
    "    'O': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "# Create the dataset object\n",
    "train_dataset = NERDataset(train_sent, train_label)\n",
    "valid_dataset = NERDataset(valid_sent, valid_label)\n",
    "test_dataset = NERDataset(test_sent, test_label)\n",
    "tiny_dataset = NERDataset(tiny_sent, tiny_label)\n",
    "\n",
    "def collate_fn(list_items):\n",
    "     x = []\n",
    "     y = []\n",
    "     for x_, y_ in list_items:\n",
    "         x.append(x_)\n",
    "         y.append(y_)\n",
    "     return x, y\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)\n",
    "tiny_loader = data.DataLoader(tiny_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_count = defaultdict(lambda: 0)\n",
    "for l in train_label:\n",
    "  for ll in l:\n",
    "    label_count[label2id[ll]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id.keys()), ignore_mismatched_sizes=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params.lr)\n",
    "\n",
    "weights = torch.ones(len(label2id))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "f1_func = torcheval.metrics.functional.multiclass_f1_score\n",
    "acc_func = torcheval.metrics.functional.multiclass_accuracy\n",
    "\n",
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "best_val_loss = float('inf')  \n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (sentences, target) in tqdm(enumerate(train_loader)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sentences_splited_into_words = [sentence.split(\" \") for sentence in sentences]\n",
    "        tokens = tokenizer.batch_encode_plus(sentences_splited_into_words, padding=True, return_tensors='pt', truncation=True, is_split_into_words=True)\n",
    "        tokens = tokens.to(device)\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "        loss = 0.0\n",
    "        for bb in range(outputs.logits.shape[0]):\n",
    "            ob = outputs.logits[bb].to(device)\n",
    "            words_ids = torch.tensor([x for x in tokens.word_ids(bb) if x is not None]).to(device)\n",
    "            predicted_class = ob[words_ids].to(device)\n",
    "            real_class = target[bb].to(device)[words_ids]\n",
    "            loss += loss_fn(predicted_class, real_class)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(\"    Train Loss: \", running_loss / len(train_loader))\n",
    "\n",
    "    ######################\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_f1 = 0.0\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (val_sentences, val_target) in tqdm(enumerate(valid_loader)):\n",
    "            val_sentences_splited_into_words = [sentence.split(\" \") for sentence in val_sentences]\n",
    "            val_tokens = tokenizer.batch_encode_plus(val_sentences_splited_into_words, padding=True, return_tensors='pt', truncation=True, is_split_into_words=True)\n",
    "            val_tokens = val_tokens.to(device)\n",
    "            val_outputs = model(**val_tokens)\n",
    "\n",
    "            val_loss_batch = 0.0\n",
    "            val_acc_batch = 0.0\n",
    "            val_f1_batch = 0.0\n",
    "            for bb in range(val_outputs.logits.shape[0]):\n",
    "                ob = val_outputs.logits[bb]\n",
    "                words_ids = torch.tensor([x for x in val_tokens.word_ids(bb) if x is not None])\n",
    "                predicted_class = ob[words_ids].to(device)\n",
    "                real_class = val_target[bb].to(device)[words_ids]\n",
    "\n",
    "                val_loss_batch = val_loss_batch + loss_fn(predicted_class, real_class)\n",
    "                val_acc_batch = acc_func(predicted_class, real_class)\n",
    "                val_f1_batch = f1_func(predicted_class, real_class)\n",
    "\n",
    "            val_loss += val_loss_batch.item()\n",
    "            val_acc += val_acc_batch.item()\n",
    "            val_f1 += val_f1_batch.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    avg_val_acc = val_acc / len(valid_loader)\n",
    "    avg_val_f1 = val_f1 / len(valid_loader)\n",
    "    print(f\"    Validation Loss: {avg_val_loss}, acc: {avg_val_acc}, f1: {avg_val_f1}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"    Best model saved with Validation Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT + LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torcheval.metrics\n",
    "from utils import parse_dataset, Params, NERDataset, get_evaluation_pred_and_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = './data/'\n",
    "training = 'train_data_ner.csv'\n",
    "test = 'test_data_ner.csv'\n",
    "tiny = 'tiny_test.csv'\n",
    "\n",
    "full_train, full_train_label = parse_dataset(base_folder + training)\n",
    "len_train = int(len(full_train) * 0.9)\n",
    "\n",
    "train_sent, train_label = full_train[:len_train], full_train_label[:len_train]\n",
    "valid_sent, valid_label = full_train[len_train + 1:], full_train_label[len_train + 1:]\n",
    "\n",
    "test_sent, test_label = parse_dataset(base_folder + test)\n",
    "tiny_sent, tiny_label = parse_dataset(base_folder + tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 80,\n",
    "    'lr': 5e-5,\n",
    "    'shuffle': False,\n",
    "    'weight_decay': 1e-4,\n",
    "    'embeddings_dim': 1024,\n",
    "}\n",
    "\n",
    "params = Params(PARAMS)\n",
    "\n",
    "label2id = {\n",
    "    'B-art': 0,\n",
    "    'B-eve': 1,\n",
    "    'B-geo': 2,\n",
    "    'B-gpe': 3,\n",
    "    'B-nat': 4,\n",
    "    'B-org': 5,\n",
    "    'B-per': 6,\n",
    "    'B-tim': 7,\n",
    "    'I-art': 8,\n",
    "    'I-eve': 9,\n",
    "    'I-geo': 10,\n",
    "    'I-gpe': 11,\n",
    "    'I-nat': 12,\n",
    "    'I-org': 13,\n",
    "    'I-per': 14,\n",
    "    'I-tim': 15,\n",
    "    'O': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "# Create the dataset object\n",
    "train_dataset = NERDataset(train_sent, train_label)\n",
    "valid_dataset = NERDataset(valid_sent, valid_label)\n",
    "test_dataset = NERDataset(test_sent, test_label)\n",
    "tiny_dataset = NERDataset(tiny_sent, tiny_label)\n",
    "\n",
    "def collate_fn(list_items):\n",
    "     x = []\n",
    "     y = []\n",
    "     for x_, y_ in list_items:\n",
    "         x.append(x_)\n",
    "         y.append(y_)\n",
    "     return x, y\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)\n",
    "tiny_loader = data.DataLoader(tiny_dataset, batch_size=params.batch_size, shuffle=params.shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_count = defaultdict(lambda: 0)\n",
    "for l in train_label:\n",
    "  for ll in l:\n",
    "    label_count[label2id[ll]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id.keys()), ignore_mismatched_sizes=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params.lr)\n",
    "\n",
    "weights = torch.ones(len(label2id))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "f1_func = torcheval.metrics.functional.multiclass_f1_score\n",
    "acc_func = torcheval.metrics.functional.multiclass_accuracy\n",
    "\n",
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "best_val_loss = float('inf') \n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (sentences, target) in tqdm(enumerate(train_loader)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sentences_splited_into_words = [sentence.split(\" \") for sentence in sentences]\n",
    "        tokens = tokenizer.batch_encode_plus(sentences_splited_into_words, padding=True, return_tensors='pt', truncation=True, is_split_into_words=True)\n",
    "        tokens = tokens.to(device)\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "        loss = 0.0\n",
    "        for bb in range(outputs.logits.shape[0]):\n",
    "            ob = outputs.logits[bb].to(device)\n",
    "            words_ids = torch.tensor([x for x in tokens.word_ids(bb) if x is not None]).to(device)\n",
    "            predicted_class = ob[words_ids].to(device)\n",
    "            real_class = target[bb].to(device)[words_ids]\n",
    "            loss += loss_fn(predicted_class, real_class)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(\"    Train Loss: \", running_loss / len(train_loader))\n",
    "\n",
    "    ######################\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_f1 = 0.0\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (val_sentences, val_target) in tqdm(enumerate(valid_loader)):\n",
    "            val_sentences_splited_into_words = [sentence.split(\" \") for sentence in val_sentences]\n",
    "            val_tokens = tokenizer.batch_encode_plus(val_sentences_splited_into_words, padding=True, return_tensors='pt', truncation=True, is_split_into_words=True)\n",
    "            val_tokens = val_tokens.to(device)\n",
    "            val_outputs = model(**val_tokens)\n",
    "\n",
    "            val_loss_batch = 0.0\n",
    "            val_acc_batch = 0.0\n",
    "            val_f1_batch = 0.0\n",
    "            for bb in range(val_outputs.logits.shape[0]):\n",
    "                ob = val_outputs.logits[bb]\n",
    "                words_ids = torch.tensor([x for x in val_tokens.word_ids(bb) if x is not None])\n",
    "                predicted_class = ob[words_ids].to(device)\n",
    "                real_class = val_target[bb].to(device)[words_ids]\n",
    "\n",
    "                val_loss_batch = val_loss_batch + loss_fn(predicted_class, real_class)\n",
    "                val_acc_batch = acc_func(predicted_class, real_class)\n",
    "                val_f1_batch = f1_func(predicted_class, real_class)\n",
    "\n",
    "            val_loss += val_loss_batch.item()\n",
    "            val_acc += val_acc_batch.item()\n",
    "            val_f1 += val_f1_batch.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    avg_val_acc = val_acc / len(valid_loader)\n",
    "    avg_val_f1 = val_f1 / len(valid_loader)\n",
    "    print(f\"    Validation Loss: {avg_val_loss}, acc: {avg_val_acc}, f1: {avg_val_f1}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"    Best model saved with Validation Loss: {best_val_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
